# -*- coding: utf-8 -*-
"""dnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ctAq0Ya_F1O7Jc76jD2gw6zewR3SMn5K
"""

import matplotlib.image as img
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from skimage import data, color
from skimage.transform import rescale, resize, downscale_local_mean
from keras.datasets import mnist

#most recent change was of sigmoid to softmax and X1_train to X1_ev
#most recent change was in relu and backward propagation 
    #try bringing cost below ~3.233333

(X_train, Y_train), (X_test, Y_test) = mnist.load_data()
X_train.shape

"""count=0;
for j in range(0,3):
    print("j : "+str(j))
    y1 = X_train[0:,j]
    print(y1)
    for i in y1:
        #print(y1[i])
        if np.isnan(i):
            print(count)
        count+=1
"""

X_test.shape

X1_test = X_test.reshape((10000,784))

Y_test.shape

Y_test = np.array([Y_test])
Y_test.shape

X1_test.shape



X1_train = X_train.reshape((60000,784))

X1_train.shape

Y_train.shape

Y_train = np.array([Y_train])
Y_train.shape

Y_train.shape

for i in range(60000):
    l = np.zeros([1,10],int)
    for j in range(0,10):
        if j==Y_train[0][i]:
            l[0][j]=1
    if i==0:
        print(l)
        print(Y_train[0][0])
        Y = np.array([l])
    else:
        Y = np.concatenate((Y,[l]),axis = 1)
print(Y.shape)

Y.shape

Y1 = Y.reshape((60000,10))
Y1.shape

Y1_train = Y1.T

Y1_train[0:,0]

X_train.shape

X1_train = X1_train.T

X1_train.shape

def sigmoid(z):
    #ma = np.max(z,axis=1,keepdims=True)
    #print(ma.size)
    #y= 1/(np.sum(np.exp(z-ma))) * np.exp(z-ma)
    #rint(y)
    #return y
    return 1/(1+np.exp(-z))

def relu(z,val):
    if val:
        for i in range(z.shape[0]):
            for j in range(z.shape[1]):
                z[i][j] = max(0,1)
    else:
        for i in range(z.shape[0]):
            for j in range(z.shape[1]):
                z[i][j] = max(0,z[i][j])
    return z

def initialize(size,s):
    w1 = np.random.randn(s+10,size)*0.01
    w2 = np.random.randn(s,s+10)*0.01
    w3 = np.random.randn(10,s)*0.01
    b1 = np.zeros([s+10,1],int)
    b2 = np.zeros([s,1],int)
    b3 = np.zeros([10,1],int)
    return (w1,w2,w3,b1,b2,b3)

def propagate(w1,w2,w3,b1,b2,b3,X,Y,sign):
    #print(w1,w2,w3,b1,b2,b3)
    z1 = np.dot(w1,X)+b1
    A1 = relu(z1,False)
    z2 = np.dot(w2,A1)+b2
    A2 = relu(z2,False)
    z3 = np.dot(w3,A2)+b3
    A3 = sigmoid(z3)
    m = X.shape[1]
    if sign==True:
        cost = np.sum((np.dot(Y,(np.log(A3).T))+np.dot((1-Y),(np.log(1-A3).T))),axis =1 )/m
        print("cost  : " +str(cost))
    dz3 = A3 - Y
    dw3 = np.dot(dz3,A2.T)/m
    db3 = dz3.sum(axis=1,keepdims=True)/m
    dz2 = np.dot(w3.T,dz3)*relu(z2,True)
    dw2 = np.dot(dz2,A1.T)/m
    db2 = dz2.sum(axis =1,keepdims=True)
    dz1 = np.dot(w2.T,dz2)*relu(z1,True)
    dw1 = np.dot(dz1,X.T)/m
    db1 = dz1.sum(axis=1,keepdims=True)
    return (dw1,dw2,dw3,db1,db2,db3)

w1,w2,w3,b1,b2,b3 = initialize(784,5)
propagate(w1,w2,w3,b1,b2,b3,X1_train,Y1_train,True)

def train(X,Y,num_iter,learning_rate,num_act_node):
    w1,w2,w3,b1,b2,b3 = initialize(784,num_act_node)
    count =0
    Vdw1 =0
    Sdw1 = 0
    Vdw2 =0
    Sdw2 =0
    Vdw3 =0
    Sdw3=0
    Vdb1 =0
    Sdb1 = 0
    Vdb2 =0
    Sdb2 =0
    Vdb3 =0
    Sdb3=0
    beta1 =0.9
    beta2 = 0.999
    for i in range(1,num_iter): 
        #print("iteration ....................................:   "+ str(i))
        if count==2:
            #learning_rate = learning_rate - 0.000015*learning_rate
            dw1,dw2,dw3,db1,db2,db3 = propagate(w1,w2,w3,b1,b2,b3,X,Y,True)
            count=0
        else:
            dw1,dw2,dw3,db1,db2,db3 = propagate(w1,w2,w3,b1,b2,b3,X,Y,False)
        Vdw1 = beta1*Vdw1 + (1-beta1)*dw1
        Sdw1 = beta2*Sdw1 + (1-beta2)*dw1**2
        w1 = w1 - learning_rate*Vdw1/np.sqrt(Sdw1)
        #print("w1")
        #print(w1)
        Vdw2 = beta1*Vdw2 + (1-beta1)*dw2
        Sdw2 = beta2*Sdw2 + (1-beta2)*dw2**2
        w2 = w2 - learning_rate*Vdw2/np.sqrt(Sdw2)
        #print("w2")
        #print(w2)
        Vdw3 = beta1*Vdw3 + (1-beta1)*dw3
        Sdw3 = beta2*Sdw3 + (1-beta2)*dw3**2
        w3 = w3 - learning_rate*Vdw3/np.sqrt(Sdw3)
        #print("w3")
        #print(w3)
        Vdb1 = beta1*Vdb1 + (1-beta1)*db1
        Sdb1 = beta2*Sdb1 + (1-beta2)*db1**2
        b1 = b1 - learning_rate*Vdb1/np.sqrt(Sdb1)
        #print("b1")
        #print(b1)
        Vdb2 = beta1*Vdb2 + (1-beta1)*db2
        Sdb2 = beta2*Sdb2 + (1-beta2)*db2**2
        b2 = b2 - learning_rate*Vdb2/np.sqrt(Sdb2)
        #print("b2")
        #print(b2)
        Vdb3 = beta1*Vdb3 + (1-beta1)*db3
        Sdb3 = beta2*Sdb3 + (1-beta2)*db3**2
        b3 = b3 - learning_rate*Vdb3/np.sqrt(Sdb3)
        #print("b3")
        #print(b3)
        count = count+1
    return (w1,w2,w3,b1,b2,b3)

"""w1,w2,w3,b1,b2,b3 = train(X1_train/255,Y1_train,5000,0.005,3)

w1,w2,w3,b1,b2,b3 = train(X1_train/255,Y1_train,5000,0.01,3)

w1
"""

mean = X1_train.sum(axis=1)/X1_train.shape[1]
var = (X1_train**2).sum(axis=1)/X1_train.shape[1]
mean+=10**-7
var+=10**-7

mean = np.array([(mean+10**-7)])
var = np.array([(var+10**-7)])
mean.shape
var.shape

mean = mean.T
var = var.T

X1_ev = ((X1_train-mean))

w1,w2,w3,b1,b2,b3 = train(X1_train,Y1_train,2000,0.005,100)





print(X1_train.shape,Y_train.shape)

def test(w1,w2,w3,b1,b2,b3,X):
    z1 = np.dot(w1,X)+b1
    A1 = tanh(z1)
    z2 = np.dot(w2,A1)+b2
    A2 = tanh(z2)
    z3 = np.dot(w3,A2)+b3
    A3 = sigmoid(z3)
    return A3

x = X1_train[:,1]

x = np.array([x])
x.shape

x = x.T

test(w1,w2,w3,b1,b2,b3,x)

Y_train[0][1]

